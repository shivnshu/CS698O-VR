{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Custom Networks\n",
    "In this notebook you have to create a custom network whose architecture has been given, and use the dataset you created earlier to train and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Statements\n",
    "#\n",
    "# Several of the imports you will need have been added but you will need to provide the\n",
    "# rest yourself; you should be able to figure out most of the imports as you go through\n",
    "# the notebook since without proper imports your code will fail to run\n",
    "#\n",
    "# All import statements go in this block\n",
    "\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All hyper parameters go in the next block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_epochs = 5\n",
    "learning_rate = 0.01\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Custom Dataset and Loader\n",
    "This is the same as part 1. Simply use the same code to create the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CDATA(torch.utils.data.Dataset): # Extend PyTorch's Dataset class\n",
    "    def __init__(self, root_dir, train, transform=None):\n",
    "        # root_dir  - the root directory of the dataset\n",
    "        # train     - a boolean parameter representing whether to return the training set or the test set\n",
    "        # transform - the transforms to be applied on the images before returning them\n",
    "        #\n",
    "        # In this function store the parameters in instance variables and make a mapping\n",
    "        # from images to labels and keep it as an instance variable. Make sure to check which\n",
    "        # dataset is required; train or test; and create the mapping accordingly.\n",
    "        \n",
    "        '''\n",
    "        for c in dir_names:\n",
    "            names = os.listdir(root_dir + dir1 + c)\n",
    "            N = len(names)\n",
    "            for n in range(N):\n",
    "                temp = mpimg.imread(root_dir + dir1 + c + '/' + names[n])\n",
    "                temp = np.array([temp])\n",
    "                images = np.concatenate((images,temp),axis=0)\n",
    "                labels += [count]\n",
    "            count+=1\n",
    "            \n",
    "        images = np.delete(images,0, axis=0)\n",
    "        self.images = images\n",
    "        '''\n",
    "        #images = np.full((1,28,28),1.0)\n",
    "        #img_names+= str(root_dir + dir1 + c + '/' + names[n])\n",
    "        #labels+=[count]*len(names)\n",
    "            \n",
    "        \n",
    "        self.root_dir = root_dir\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        if train == True:\n",
    "            dir1 = '/train/'\n",
    "        else:\n",
    "            dir1 = '/test/'\n",
    "        \n",
    "        dir_names = os.listdir(root_dir + dir1)\n",
    "        img_names=[]\n",
    "        labels = []\n",
    "        count = 0\n",
    "        for c in dir_names[0:num_classes]:\n",
    "            names = os.listdir(root_dir + dir1 + c)\n",
    "            N = len(names)\n",
    "            for n in range(N):\n",
    "                img_names.append(str(root_dir + dir1 + c + '/' + names[n]))\n",
    "                labels += [count]\n",
    "            count+=1\n",
    "               \n",
    "        \n",
    "        self.img_names = img_names\n",
    "        self.labels = labels\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        # return the size of the dataset (total number of images) as an integer\n",
    "        # this should be rather easy if you created a mapping in __init__\n",
    "        \n",
    "        return len(self.img_names)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # idx - the index of the sample requested\n",
    "        # Open the image correspoding to idx, apply transforms on it and return a tuple (image, label)\n",
    "        # where label is an integer from 0-9 (since notMNIST has 10 classes)\n",
    "        #temp = mpimg.imread(self.img_names[idx])\n",
    "        \n",
    "        img_pil = Image.open(self.img_names[idx]).convert('RGB')\n",
    "        return self.transform(img_pil), self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    \n",
    "composed_transform = transforms.Compose([transforms.Scale((32,32)),transforms.ToTensor()])\n",
    "train_dataset = CDATA(root_dir='../a1/', train=True, transform=composed_transform) # Supply proper root_dir\n",
    "test_dataset = CDATA(root_dir='../a1/', train=False, transform=composed_transform) # Supply proper root_dir\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Custom Network\n",
    "It's time to create a new custom network. This network is based on Resnet (indeed it is a resnet since it uses skip connections). The architecture of the network is provided in the diagram. It specifies the layer names, layer types as well as their parameters.\n",
    "<img src=\"architecture.png\" width=100>\n",
    "[Full size image](architecture.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CustomResnet(nn.Module): # Extend PyTorch's Module class\n",
    "    def __init__(self, num_classes = 10):\n",
    "        super(CustomResnet, self).__init__() # Must call super __init__()\n",
    "        \n",
    "        # Define the layers of the network here\n",
    "        # There should be 17 total layers as evident from the diagram\n",
    "        # The parameters and names for the layers are provided in the diagram\n",
    "        # The variable names have to be the same as the ones in the diagram\n",
    "        # Otherwise, the weights will not load\n",
    "        \n",
    "        self.conv1 = torch.nn.Conv2d(3, 64, 7, stride=1, padding=3, bias=True)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(64)\n",
    "        self.relu =torch.nn.ReLU(True)\n",
    "        self.maxpool= torch.nn.MaxPool2d(3, stride=2, padding=1)\n",
    "        \n",
    "        self.lyr1conv1 = torch.nn.Conv2d(64, 64, 3, stride=1, padding=1, bias=True)\n",
    "        self.lyr1bn1 = torch.nn.BatchNorm1d(64)\n",
    "        self.lyr1relu1 =torch.nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.lyr1conv2 = torch.nn.Conv2d(64, 64, 3, stride=1, padding=1, bias=True)\n",
    "        self.lyr1bn2 = torch.nn.BatchNorm1d(64)\n",
    "        self.lyr1relu2 =torch.nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.lyr2conv1 = torch.nn.Conv2d(64, 64, 3, stride=1, padding=1, bias=True)\n",
    "        self.lyr2bn1 = torch.nn.BatchNorm1d(64)\n",
    "        self.lyr2relu1 =torch.nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.lyr2conv2 = torch.nn.Conv2d(64, 64, 3, stride=1, padding=1, bias=True)\n",
    "        self.lyr2bn2 = torch.nn.BatchNorm1d(64)\n",
    "        self.lyr2relu2 =torch.nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.fc = nn.Linear(4096, num_classes)\n",
    "        \n",
    "       \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Here you have to define the forward pass\n",
    "        # Make sure you take care of the skip connections\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x1 = self.maxpool(x)\n",
    "\n",
    "        x = self.lyr1conv1(x1)\n",
    "        x = self.lyr1bn1(x)\n",
    "        x = self.lyr1relu1(x)\n",
    "        \n",
    "        x = self.lyr1conv2(x)\n",
    "        x = self.lyr1bn2(x)\n",
    "        x1 = self.lyr1relu2(x+x1)\n",
    "        \n",
    "        x = self.lyr2conv1(x1)\n",
    "        x = self.lyr2bn1(x)\n",
    "        x = self.lyr2relu1(x)\n",
    "        \n",
    "        x = self.lyr2conv2(x)\n",
    "        x = self.lyr2bn2(x)\n",
    "        x = self.lyr2relu2(x+x1)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finetune on pre-trained CIFAR-100 weights\n",
    "We shall now finetune our model using pretrained CIFAR-100 weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = CustomResnet(num_classes = 100) # 100 classes since CIFAR-100 has 100 classes\n",
    "\n",
    "# Load CIFAR-100 weights. (Download them from assignment page)\n",
    "# If network was properly implemented, weights should load without any problems\n",
    "model.load_state_dict(torch.load('../CIFAR-100_weights')) # Supply the path to the weight file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optional\n",
    "As a sanity check you may load the CIFAR-100 test dataset and test the above model. You should get an accuracy of ~41%. This part is optional and is meant for your convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Block for optionally running the model on CIFAR-100 test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's finetune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change last layer to output 10 classes since our dataset has 10 classes\n",
    "model.fc = # Complete this statement. It is similar to the resnet18 case\n",
    "\n",
    "# Loss function and optimizers\n",
    "criterion = nn.CrossEntropyLoss().cuda()# Define cross-entropy loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate) # Use Adam optimizer, use learning_rate hyper parameter\n",
    "\n",
    "def train():\n",
    "    # Code for training the model\n",
    "    # Make sure to output a matplotlib graph of training losses\n",
    "\n",
    "    %time train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    # Write loops for testing the model on the test set\n",
    "    # You should also print out the accuracy of the model\n",
    "    \n",
    "%time test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training from scratch\n",
    "Now we shall try training the model from scratch and observe the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reinstantiate the model and optimizer\n",
    "model = CustomResnet(num_classes = 10)\n",
    "optimizer = # Use Adam optimizer, use learning_rate hyper parameter\n",
    "\n",
    "# Train\n",
    "%time train()\n",
    "\n",
    "# Test\n",
    "%time test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end of Assignment 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
