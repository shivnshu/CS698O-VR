{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: The Winter is here\n",
    "##### This works best with epic battle music. No spoilers present.\n",
    "<br/>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Tywin Lannister was right when he said: \"The great war is between death and life, ice and fire. If we loose, the night will never end\"<br/>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;It has been six months since the white walkers' army marched into the north, led by the night king himself on a dead dragon. It has been a battle like never before: never before have men faced such an enemy in battle, never before have men fought so bravely against a united threat, and never before have they been so gravely defeated.<br />\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; While Cersei is in King's landing, brave men have died fighting the great war. Among others, Tyrion is dead, Arya is dead and Jon Snow is dead, again. In a desperate battle, Daenerys leads all her forces in a final stand-off with the dead just south of Winterfell. <br />\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Her army defeated, she is now on the run on her dragon in an air battle, being chased by two of her own dragons, the Night king and a dead Jon Snow. Suddenly, the Night king's spear hits Danny's dragon, who, raining blood and fire, falls into ice, taking the lost queen, with him. <br />\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Daenerys opens her eyes in a strange place, a place which does not follow the rules of space and time, where the dead souls killed by the dead men are trapped, forever. But who woke her up? There stands near her, Tyrion, with Jorah, Davos, Jon Snow, and everybody else. They all indulge in a heartfelt reunion when someone yells- \"But how do we get out?<br />\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Varys sees a talking crystal close by, who asks them of completing a task, which on completion would allow them to go back to the land of the living, with the ultimate tool to defeat the white-walkers and kills the night king, the Dragon-axe. They have summoned you for help, as the task is out of their expertise, to apply a modified CNN to solve the object detection problem on the PASCAL VOC dataset. Varys, the master of whisperers, has used his talents to import the following for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, unicode_literals\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import torchvision\n",
    "import random\n",
    "from collections import namedtuple\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.ion()\n",
    "# You can ask Varys to get you more if you desire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cersei chose violence, you choose your hyper-parameters wisely using validation data!\n",
    "resnet_input = 224 \n",
    "batch_size = 2\n",
    "num_epochs = 5\n",
    "learning_rate =  0.001\n",
    "hyp_momentum = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Build the data\n",
    "The hound who was in charge for getting the data, brought you the following links:\n",
    "<br/>Training and validation:\n",
    "<br/>http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\n",
    "<br/>Testing data:\n",
    "<br/>http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar\n",
    "<br/>He also told you that the dataset(datascrolls :P) consists of images from of 20 classes, with detection annotations included. The JPEGImages folder houses the images, and the Annotations folder has the object-wise labels for the objects in one xml file per image. You have to extract the object information, ie. the [xmin, ymin] (the top left x,y co-ordinates) and the [xmax, ymax] (the bottom right x,y co-ordinates) of only the objects belonging to the given 20 classes(aeroplane, bicycle, boat, bottle, bus, car, cat, chair, cow, dining table, dog, horse, motorbike, person, potted plant, sheep, train, TV). For parsing the xml file, you can ask Varys to import xml.etree.ElementTree for you. <br/>\n",
    "<br/> You can then ask Bronn and Jamie to organize the data as follows:\n",
    "<br/> For every image in the dataset, extract/crop the object patch from the image one by one using their respective co-ordinates:[xmin, ymin, xmax, ymax], resize the image to resnet_input, and store it with its class label information. Do the same for training/validation and test datasets. <br/>\n",
    "##### Important\n",
    "You also have to collect data for an extra background class which stands for the class of an object which is not a part of any of the 20 classes. For this, you can crop and resize any random patches from an image. A good idea is to extract patches that have low \"intersection over union\" with any object present in the image frame from the 20 Pascal VOC classes. The number of background images should be roughly around those of other class objects' images. Hence the total classes turn out to be 21. This is important for applying the sliding window method later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = ('__background__', 'aeroplane', 'bicycle', 'car', 'cat', 'dog')\n",
    "\n",
    "# counts to maintain the number of training examples in each of the 6 classes\n",
    "counts = [0]*6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to create the directories corresponding to images of each of 6 classes\n",
    "\n",
    "def create_hierarchical_dirs():\n",
    "    if not os.path.exists('data/train'):\n",
    "        os.makedirs('data/train')      \n",
    "        \n",
    "    for i in range(len(classes)):\n",
    "        dirname = 'data/train/' + classes[i]\n",
    "        if not os.path.exists(dirname):\n",
    "            os.makedirs(dirname)\n",
    "\n",
    "create_hierarchical_dirs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to crop and distribute the given training data images in their corresponding class directories \n",
    "\n",
    "def jamie_bronn_build_dataset():\n",
    "    \n",
    "    dir_names = os.listdir('data/VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007/Annotations')    \n",
    "    for img_name in dir_names:\n",
    "        tree = ET.parse('data/VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007/Annotations/' + img_name)\n",
    "        root = tree.getroot()\n",
    "        for object1 in root.iter('object'):\n",
    "            for name1 in object1.iter('name'):\n",
    "                dirname = name1.text\n",
    "            for bndbox in object1.iter('bndbox'):\n",
    "                for xmin in object1.iter('xmin'):\n",
    "                    x1 = int(float(xmin.text))\n",
    "                for xmax in object1.iter('xmax'):\n",
    "                    x2 = int(float(xmax.text))\n",
    "                for ymin in object1.iter('ymin'):\n",
    "                    y1 = int(float(ymin.text))\n",
    "                for ymax in object1.iter('ymax'):\n",
    "                    y2 = int(float(ymax.text))\n",
    "\n",
    "                img = Image.open('data/VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007/JPEGImages/' + img_name[0:-4] + '.jpg')\n",
    "                img = img.crop((x1,y1,x2,y2)).resize((resnet_input,resnet_input))\n",
    "                if dirname in classes:\n",
    "                    img.save('data/train/' + dirname + '/' + str(counts[classes.index(dirname)]) + \".jpg\");\n",
    "                    counts[classes.index(dirname)]+=1\n",
    "                    \n",
    "jamie_bronn_build_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to make a rectangle using four coordinates (x1,y1,x2,y2)\n",
    "\n",
    "Rectangle = namedtuple('Rectangle', 'xmin ymin xmax ymax')\n",
    "\n",
    "# Function to calculate the overlappig area of 2 given rectangles\n",
    "\n",
    "def area(a, b):  # returns None if rectangles don't intersect\n",
    "    dx = min(a.xmax, b.xmax) - max(a.xmin, b.xmin)\n",
    "    dy = min(a.ymax, b.ymax) - max(a.ymin, b.ymin)\n",
    "    if (dx>=0) and (dy>=0):\n",
    "        return dx*dy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create negative samples i.e. background class images\n",
    "\n",
    "def build_background_class():\n",
    "    dir_names = os.listdir('data/VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007/Annotations')\n",
    "    average = int(sum(counts[1:])/len(counts[1:]))\n",
    "    counts[0] = 0\n",
    "    while(counts[0]!=average): \n",
    "        select = 1\n",
    "        img_name = random.choice(dir_names)\n",
    "        tree = ET.parse('data/VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007/Annotations/' + img_name)\n",
    "        root = tree.getroot()\n",
    "        img = Image.open('data/VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007/JPEGImages/' + img_name[0:-4] + '.jpg')\n",
    "        imsize = img.size;\n",
    "        x1 = random.randrange(imsize[0])\n",
    "        x2 = random.randrange(imsize[0])\n",
    "        y1 = random.randrange(imsize[1])\n",
    "        y2 = random.randrange(imsize[1])\n",
    "        if(x1>x2):\n",
    "            temp = x1\n",
    "            x1=x2\n",
    "            x2=temp\n",
    "        if(y1>y2):\n",
    "            temp = y1\n",
    "            y1=y2\n",
    "            y2=temp\n",
    "        if(x1==x2 or y1==y2):\n",
    "            continue\n",
    "        for object1 in root.iter('object'):\n",
    "            for name1 in object1.iter('name'):\n",
    "                dirname = name1.text\n",
    "            for bndbox in object1.iter('bndbox'):\n",
    "                intersection = 0\n",
    "                for xmin in object1.iter('xmin'):\n",
    "                    xa = int(float(xmin.text))\n",
    "                for xmax in object1.iter('xmax'):\n",
    "                    xb = int(float(xmax.text))\n",
    "                for ymin in object1.iter('ymin'):\n",
    "                    ya = int(float(ymin.text))\n",
    "                for ymax in object1.iter('ymax'):\n",
    "                    yb = int(float(ymax.text))\n",
    "                ra = Rectangle(x1, x2, y1, y2)\n",
    "                rb = Rectangle(xa, xb, ya, yb)\n",
    "                if(area(ra,rb)!=None):\n",
    "                    intersection = area(ra,rb)\n",
    "                union = ((x2-x1)*(y2-y1))+((xb-xa)*(yb-ya))-intersection\n",
    "                if(float(intersection)/union>0.5):\n",
    "                    select = 0\n",
    "\n",
    "        if(select==1):\n",
    "            img = img.crop((x1,y1,x2,y2)).resize((resnet_input,resnet_input))\n",
    "            img.save('data/train/__background__/' + str(counts[0]) + \".jpg\");\n",
    "            counts[0] += 1\n",
    "    \n",
    "build_background_class()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to build train_loader ad test_loader\n",
    "\n",
    "# counts to maintain the number of training examples in each of the 6 classes\n",
    "counts = [0]*6\n",
    "\n",
    "class hound_dataset(torch.utils.data.Dataset): # Extend PyTorch's Dataset class\n",
    "    def __init__(self, root_dir, train, transform=None):\n",
    "        # Begin\n",
    "        \n",
    "        \n",
    "        if (train==True): \n",
    "            \n",
    "            # In case of train_loader, return images and their corresponding labels\n",
    "            \n",
    "            dir1 = '/train/'\n",
    "            dir_names = os.listdir(root_dir + dir1)\n",
    "            img_names=[]\n",
    "            labels = []\n",
    "            count = 0\n",
    "            for temp in range(len(dir_names)):\n",
    "                names = os.listdir(root_dir + dir1 + classes[temp])\n",
    "                N = len(names)\n",
    "                for n in range(N):\n",
    "                    img_names.append(str(root_dir + dir1 + classes[temp] + '/' + names[n]))\n",
    "                    labels += [temp]\n",
    "                    counts[temp] += 1\n",
    "            self.labels = labels \n",
    "         \n",
    "        \n",
    "        \n",
    "        else:\n",
    "            \n",
    "            # In case of test_loader, return images and their corresponding xml file_names. These xml_names are later\n",
    "            # used to extract the ground-truth bouning boxes and calculate the accuracy of the model\n",
    "            \n",
    "            img_names=[]\n",
    "            xml_names=[]\n",
    "            dir1 = '/VOCtest_06-Nov-2007/VOCdevkit/VOC2007/JPEGImages'\n",
    "            dir2 = '/VOCtest_06-Nov-2007/VOCdevkit/VOC2007/Annotations'\n",
    "            dir_names = os.listdir(root_dir + dir1)\n",
    "            for name in dir_names:\n",
    "                img_names.append(str(root_dir + dir1 + '/' + name))\n",
    "                xml_names.append(str(root_dir + dir2 + '/' + name[:-4] + '.xml'))\n",
    "            self.xml_names = xml_names\n",
    "                \n",
    "        self.img_names = img_names    \n",
    "        self.train = train\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        \n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        # Begin\n",
    "        \n",
    "        return len(self.img_names)        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # Begin      \n",
    "        \n",
    "        img_pil = Image.open(self.img_names[idx])\n",
    "        if(self.train==True):\n",
    "            # In case of train_loader, return images and their corresponding labels            \n",
    "            return self.transform(img_pil), self.labels[idx]\n",
    "        else:\n",
    "            # In case of test_loader, return images and their corresponding xml file_names. These xml_names are later\n",
    "            # used to extract the ground-truth bouning boxes and calculate the accuracy of the model            \n",
    "            return self.transform(img_pil), self.xml_names[idx]     \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network\n",
    "<br/>You can ask Arya to train the network on the created dataset. This will yield a classification network on the 21 classes of the VOC dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "composed_transform = transforms.Compose([transforms.Scale((resnet_input,resnet_input)),\n",
    "                                         transforms.RandomHorizontalFlip(),transforms.ToTensor()])\n",
    "train_dataset = hound_dataset(root_dir='./data', train=True, transform=composed_transform) # Supply proper root_dir\n",
    "test_dataset = hound_dataset(root_dir='./data', train=False, transform=composed_transform) # Supply proper root_dir\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning\n",
    "Litlefinger has brought you a pre-trained network. Fine-tune the network in the following section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet (\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (relu): ReLU (inplace)\n",
       "  (maxpool): MaxPool2d (size=(3, 3), stride=(2, 2), padding=(1, 1), dilation=(1, 1))\n",
       "  (layer1): Sequential (\n",
       "    (0): BasicBlock (\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU (inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "    (1): BasicBlock (\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU (inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential (\n",
       "    (0): BasicBlock (\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU (inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (downsample): Sequential (\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock (\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU (inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential (\n",
       "    (0): BasicBlock (\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU (inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (downsample): Sequential (\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock (\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU (inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential (\n",
       "    (0): BasicBlock (\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU (inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (downsample): Sequential (\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock (\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU (inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d (size=7, stride=7, padding=0, ceil_mode=False, count_include_pad=True)\n",
       "  (fc): Linear (512 -> 6)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet18 = models.resnet18(pretrained=True)\n",
    "\n",
    "for param in resnet18.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "resnet18.fc = nn.Linear(resnet18.fc.in_features, 6)\n",
    "resnet18.cuda()\n",
    "\n",
    "# Add code fors using CUDA here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating weights for each of the 6 classes to account for the imbalance in the amount of training examples \n",
    "# of each of these classes\n",
    "\n",
    "weights = [0]*6\n",
    "total_images = sum(counts)\n",
    "for i in range(len(weights)):\n",
    "    weights[i] = total_images/counts[i]\n",
    "weights = torch.FloatTensor(weights)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=weights).cuda()\n",
    "optimizer = optim.SGD(resnet18.fc.parameters(), learning_rate, hyp_momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training the model\n",
    "\n",
    "def arya_train():\n",
    "    # Begin\n",
    "    for epoch in range(num_epochs): \n",
    "        resnet18.train()        \n",
    "        for i, (input1,target) in enumerate(train_loader):\n",
    "            \n",
    "            input_var = torch.autograd.Variable(input1).cuda()            \n",
    "            target = target.cuda(async = True)\n",
    "            target_var = torch.autograd.Variable(target)\n",
    "            \n",
    "            output = resnet18(input_var)\n",
    "            loss = criterion(output, target_var)           \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            output = output.float()\n",
    "            loss = loss.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 6s, sys: 1min 39s, total: 8min 46s\n",
      "Wall time: 4min 25s\n"
     ]
    }
   ],
   "source": [
    "%time arya_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Saving the trained model parameters\n",
    "\n",
    "torch.save(resnet18.state_dict(), 'params.pkl')\n",
    "#resnet18.load_state_dict(torch.load('params.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculating the training accuracy\n",
    "\n",
    "def train_accuracy():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for i, (input1,target) in enumerate(train_loader):\n",
    "            \n",
    "            input_var = torch.autograd.Variable(input1,volatile = True).cuda()\n",
    "            target = target.cuda(async = True)\n",
    "            target_var = torch.autograd.Variable(target)\n",
    "                        \n",
    "            output = resnet18(input_var)\n",
    "            output = output.float()\n",
    "            \n",
    "            # t1 gives the array of predicted classes and t2 is the array of original class \n",
    "            t1 = output.max(1)[1].cpu().data.numpy()\n",
    "            t2 = target_var.cpu().data.numpy()\n",
    "            \n",
    "            # Updating both the total and correct by adding the total no. of images in this batch as well as the\n",
    "            # correctly classified images in this batch\n",
    "            correct += sum(t1==t2)\n",
    "            total += len(t2)\n",
    "            \n",
    "    print('Train Accuracy: '+str(correct*100.0/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 68.1726907631\n",
      "CPU times: user 1min 39s, sys: 28.5 s, total: 2min 8s\n",
      "Wall time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "%time train_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing and Accuracy Calculation\n",
    "Jorah then asks a question, how is this a detection task?<br/>\n",
    "As everybody wonders, Theon Greyjoy suggests a slding window method to test the above trained trained network on the detection task:<br/>\n",
    "\"We take some windows of varying size and aspect ratios\", he mumbled, \"and slide it through the test image (considering some stride of pixels) from left to right, and top to bottom, detect the class scores for each of the window, and keep only those which are above a certain threshold value!\". \"He is right\", says Samwell, \"I read a similar approach in the paper -Faster RCNN by Ross Girshick in the library, where he uses three diferent scales/sizes and three different aspect ratios, making a total of nine windows per pixel to slide\". You need to write the code and use it in testing code to find the predicted boxes and their classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sliding windows per image:  4761\n"
     ]
    }
   ],
   "source": [
    "def theon_sliding_window():\n",
    "    # Generating a range of sliding windows using 3 different sizes and 3 different aspect ratios\n",
    "    # and returning the corresponding rectangle coordinates\n",
    "    \n",
    "    boxes=[]\n",
    "    imres = [[192,192],[192,96],[96,192],[128,128],[128,64],[64,128],[64,64],[64,32],[32,64]]\n",
    "    for i in range(0,resnet_input,10):\n",
    "        for j in range(0,resnet_input,10):\n",
    "            for window in imres:\n",
    "                boxes.append([i-window[0]/2, j-window[1]/2, i+window[0]/2, j+window[1]/2])\n",
    "    return np.asarray(np.array(boxes))\n",
    "\n",
    "sliding_windows = theon_sliding_window()\n",
    "print('Total number of sliding windows per image:  ' + str(len(sliding_windows)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def aegon_targaryen_non_maximum_supression(boxes,threshold = 0.7):\n",
    "        \n",
    "        boxes = np.array(boxes)\n",
    "        final_boxes = []\n",
    "\n",
    "        # Extracting th x1,y1,x2,y2 co-ordinates from the boes array\n",
    "        x1 = boxes[:,0]\n",
    "        y1 = boxes[:,1]\n",
    "        x2 = boxes[:,2]\n",
    "        y2 = boxes[:,3]\n",
    "\n",
    "        # Calculating area for each of the (x1,y1,x2,y2) tuple in the boxes list\n",
    "        area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "        idxs = np.argsort(y2)\n",
    "        \n",
    "        \n",
    "        # Selecting a box and deleting boxes in the list that have greater than some threshold overlap \n",
    "        # with the selected box\n",
    "        \n",
    "        while len(idxs) > 0:\n",
    "            \n",
    "            last = len(idxs) - 1\n",
    "            i = idxs[last]\n",
    "            final_boxes.append(i)\n",
    "            suppress = [last]\n",
    "            \n",
    "            for pos in range(0, last):\n",
    "                \n",
    "                j = idxs[pos]\n",
    "                \n",
    "                ra = Rectangle(x1[i],y1[i],x2[i],y2[i])\n",
    "                rb = Rectangle(x1[j],y1[j],x2[j],y2[j])\n",
    "                intersection = area(ra,rb)\n",
    "                \n",
    "                if(area(ra,rb) == None):\n",
    "                    overlap = 0\n",
    "                else:\n",
    "                    overlap = intersection / area[j]\n",
    "\n",
    "                if overlap > threshold:\n",
    "                    suppress.append(pos)\n",
    "\n",
    "            idxs = np.delete(idxs, suppress)\n",
    "\n",
    "        return boxes[final_boxes]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Wait\", says <b>Jon Snow</b>, \"The predicted boxes may be too many and we can't deal with all of them. So, I myself will go and apply non_maximum_supression to reduce the number of boxes\". You are free to choose the threshold value for non maximum supression, but choose wisely [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to return the list of ground-truth bounding boxes in an image given its xml file_name\n",
    "\n",
    "def retrieve_bbox_list(xml):\n",
    "    \n",
    "    bbox_list = []\n",
    "    tree = ET.parse(xml)\n",
    "    root = tree.getroot()\n",
    "    for object1 in root.iter('object'):\n",
    "        for name1 in object1.iter('name'):\n",
    "            classname = name1.text                        \n",
    "        for bndbox in object1.iter('bndbox'):\n",
    "            for xmin in object1.iter('xmin'):\n",
    "                x1 = int(float(xmin.text))\n",
    "            for xmax in object1.iter('xmax'):\n",
    "                x2 = int(float(xmax.text))\n",
    "            for ymin in object1.iter('ymin'):\n",
    "                y1 = int(float(ymin.text))\n",
    "            for ymax in object1.iter('ymax'):\n",
    "                y2 = int(float(ymax.text))\n",
    "            if(classes.__contains__(classname)):\n",
    "                bbox_list.append([x1,y1,x2,y2,classes.index(classname)])  \n",
    "    return bbox_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to test the accuracy of the model\n",
    "\n",
    "temp = np.empty((3,224,224))\n",
    "def daenerys_test():\n",
    "    \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    \n",
    "    transform_PIL = transforms.Compose([torchvision.transforms.ToPILImage()])\n",
    "    \n",
    "    for i, (inputvar ,xml_names) in enumerate(test_loader):\n",
    "            \n",
    "            # Iterating on each image in the given batch and extracting the cropped versions of each image corresponding\n",
    "            # to each sliding window, then feeding each of these into the network and appyling the mean-shift-clustering \n",
    "            # technique to obtain the final detection windows\n",
    "            \n",
    "            for iter1 in range(len(inputvar)):\n",
    "                \n",
    "                temp = inputvar[iter1]\n",
    "                img = transform_PIL(inputvar[iter1])\n",
    "                xml = xml_names[iter1]\n",
    "                \n",
    "                gd_bbox_list = retrieve_bbox_list(xml)\n",
    "                boxes = []\n",
    "                window_count = 0\n",
    "                \n",
    "                # Iterating over each sliding window and passing the cropped versions of the image to the conv-net to\n",
    "                # obtain the class-probabilities\n",
    "                \n",
    "                for window in sliding_windows:\n",
    "                    window_count += 1\n",
    "                    input_conv = composed_transform(img.crop(window))\n",
    "                    input_var = torch.autograd.Variable(input_conv, volatile = True).cuda()\n",
    "                    input_var = input_var.unsqueeze(0)\n",
    "                    output = resnet18(input_var)\n",
    "                    output = output.float()\n",
    "                    pred = output.max(1)[1].cpu().data.numpy()                    \n",
    "                    pred_prob = output.max(1)[0].cpu().data.numpy() \n",
    "                    \n",
    "                    boxes.append([window[0],window[1],window[2],window[3],pred_prob[0],pred[0]])\n",
    "                 \n",
    "                \n",
    "                # Identifying the boxes having high probablility of containing an object(of any class)\n",
    "                \n",
    "                new_boxes = []\n",
    "                for box in boxes:\n",
    "                    if box[4]>0.3 and box[5]!=0:\n",
    "                        new_boxes.append(box)\n",
    "                boxes = new_boxes\n",
    "                \n",
    "                # Applying the non-maximal-suppression to obtain the final detection windows\n",
    "                pred_bboxes = aegon_targaryen_non_maximum_supression(boxes)\n",
    "                \n",
    "                # Calculating the accuracy of the model by calculating the number of predicted windows which have \n",
    "                # intersection over union greater than 0.5 with the ground truth windows and have the same class\n",
    "                total += len(gd_bbox_list)\n",
    "                correct_bool = [0]*total\n",
    "                for pred_bbox in pred_bboxes:\n",
    "                    for gd_bbox in gd_bbox_list:\n",
    "                        ra = Rectangle(pred_bbox[0], pred_bbox[1], pred_bbox[2], pred_bbox[3])\n",
    "                        rb = Rectangle(gd_bbox[0], gd_bbox[1], gd_bbox[2], gd_bbox[3])\n",
    "                        if(area(ra,rb)==None):\n",
    "                            continue\n",
    "                        union = (gd_bbox[3]-gd_bbox[1])*(gd_bbox[2]-gd_bbox[0]) + \\\n",
    "                                (pred_bbox[3]- pred_bbox[1])*(pred_bbox[2]- pred_bbox[0])- area(ra,rb)\n",
    "                        print(pred_bbox[5],gd_bbox[4])\n",
    "                        if (area(ra, rb) > 0.5*union and pred_bbox[5] == gd_bbox[4]):\n",
    "                            correct_bool[gd_bbox_list.index(gd_bbox)] = 1\n",
    "                correct += sum(correct_bool)\n",
    "                \n",
    "     \n",
    "    accuracy = correct/total * 100\n",
    "                \n",
    "    print('Test Accuracy: ' + str(accuracy))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Number of test images is approx. 5K. and for each image, we have to extract and process 4761 sliding windows\n",
    "# and this is taking a lot of time\n",
    "\n",
    "%time daenerys_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Showdown\n",
    "After covering all the steps and passing the accuracy value to the talking crystal, they all pass through to the land of the living, with a wounded Jon Snow armed with the Dragon-axe. After a fierce battle, Jon Snow manages to go face to face with the Night king. Surrounded by battling men and falling bodies, they engage in a ferocious battle, a battle of spear and axe. After a raging fight, Jon manages to sink the axe into the Night king's heart, but not before he gets wounded by the spear. As dead men fall to bones, Daenerys and others rush to his aid, but it is too late. Everyone is in tears as they look towards the man of honour, Jon Snow, lying in Daenerys's arms when he says his last words: \"The night has ended. Winter is finally over!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
