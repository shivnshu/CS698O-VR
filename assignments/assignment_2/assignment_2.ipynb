{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: The Winter is here\n",
    "##### This works best with epic battle music. No spoilers present.\n",
    "<br/>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Tywin Lannister was right when he said: \"The great war is between death and life, ice and fire. If we loose, the night will never end\"<br/>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;It has been six months since the white walkers' army marched into the north, led by the night king himself on a dead dragon. It has been a battle like never before: never before have men faced such an enemy in battle, never before have men fought so bravely against a united threat, and never before have they been so gravely defeated.<br />\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; While Cersei is in King's landing, brave men have died fighting the great war. Among others, Tyrion is dead, Arya is dead and Jon Snow is dead, again. In a desperate battle, Daenerys leads all her forces in a final stand-off with the dead just south of Winterfell. <br />\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Her army defeated, she is now on the run on her dragon in an air battle, being chased by two of her own dragons, the Night king and a dead Jon Snow. Suddenly, the Night king's spear hits Danny's dragon, who, raining blood and fire, falls into ice, taking the lost queen, with him. <br />\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Daenerys opens her eyes in a strange place, a place which does not follow the rules of space and time, where the dead souls killed by the dead men are trapped, forever. But who woke her up? There stands near her, Tyrion, with Jorah, Davos, Jon Snow, and everybody else. They all indulge in a heartfelt reunion when someone yells- \"But how do we get out?<br />\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Varys sees a talking crystal close by, who asks them of completing a task, which on completion would allow them to go back to the land of the living, with the ultimate tool to defeat the white-walkers and kills the night king, the Dragon-axe. They have summoned you for help, as the task is out of their expertise, to apply a modified CNN to solve the object detection problem on the PASCAL VOC dataset. Varys, the master of whisperers, has used his talents to import the following for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, unicode_literals\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.ion()\n",
    "# You can ask Varys to get you more if you desire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resnet_input = 224 #size of resnet18 input images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cersei chose violence, you choose your hyper-parameters wisely using validation data!\n",
    "batch_size = 2\n",
    "num_epochs = 5\n",
    "learning_rate =  0.001\n",
    "hyp_momentum = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Build the data\n",
    "The hound who was in charge for getting the data, brought you the following links:\n",
    "<br/>Training and validation:\n",
    "<br/>http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\n",
    "<br/>Testing data:\n",
    "<br/>http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar\n",
    "<br/>He also told you that the dataset(datascrolls :P) consists of images from of 20 classes, with detection annotations included. The JPEGImages folder houses the images, and the Annotations folder has the object-wise labels for the objects in one xml file per image. You have to extract the object information, ie. the [xmin, ymin] (the top left x,y co-ordinates) and the [xmax, ymax] (the bottom right x,y co-ordinates) of only the objects belonging to the given 20 classes(aeroplane, bicycle, boat, bottle, bus, car, cat, chair, cow, dining table, dog, horse, motorbike, person, potted plant, sheep, train, TV). For parsing the xml file, you can ask Varys to import xml.etree.ElementTree for you. <br/>\n",
    "<br/> You can then ask Bronn and Jamie to organize the data as follows:\n",
    "<br/> For every image in the dataset, extract/crop the object patch from the image one by one using their respective co-ordinates:[xmin, ymin, xmax, ymax], resize the image to resnet_input, and store it with its class label information. Do the same for training/validation and test datasets. <br/>\n",
    "##### Important\n",
    "You also have to collect data for an extra background class which stands for the class of an object which is not a part of any of the 20 classes. For this, you can crop and resize any random patches from an image. A good idea is to extract patches that have low \"intersection over union\" with any object present in the image frame from the 20 Pascal VOC classes. The number of background images should be roughly around those of other class objects' images. Hence the total classes turn out to be 21. This is important for applying the sliding window method later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = ('__background__',\n",
    "           'aeroplane', 'bicycle', 'bird', 'boat',\n",
    "           'bottle', 'bus', 'car', 'cat', 'chair',\n",
    "           'cow', 'diningtable', 'dog', 'horse',\n",
    "           'motorbike', 'person', 'pottedplant',\n",
    "           'sheep', 'sofa', 'train', 'tvmonitor')\n",
    "counts = [0]*21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes.index('aeroplane')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rectangle = namedtuple('Rectangle', 'xmin ymin xmax ymax')\n",
    "\n",
    "def area(a, b):  # returns None if rectangles don't intersect\n",
    "    dx = min(a.xmax, b.xmax) - max(a.xmin, b.xmin)\n",
    "    dy = min(a.ymax, b.ymax) - max(a.ymin, b.ymin)\n",
    "    if (dx>=0) and (dy>=0):\n",
    "        return dx*dy\n",
    "    \n",
    "ra = Rectangle(1, 1, 3, 3)\n",
    "rb = Rectangle(1, 1, 5, 5)\n",
    "\n",
    "type(area(ra,rb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_hierarchical_dirs():\n",
    "    if not os.path.exists('data/train'):\n",
    "        os.makedirs('data/train')      \n",
    "        \n",
    "    for i in range(len(classes)):\n",
    "        dirname = 'data/train/' + classes[i]\n",
    "        if not os.path.exists(dirname):\n",
    "            os.makedirs(dirname)\n",
    "\n",
    "create_hierarchical_dirs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def jamie_bronn_build_dataset():\n",
    "    \n",
    "    dir_names = os.listdir('data/VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007/Annotations')    \n",
    "    for img_name in dir_names:\n",
    "        tree = ET.parse('data/VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007/Annotations/' + img_name)\n",
    "        root = tree.getroot()\n",
    "        for object1 in root.iter('object'):\n",
    "            for name1 in object1.iter('name'):\n",
    "                dirname = name1.text\n",
    "            for bndbox in object1.iter('bndbox'):\n",
    "                for xmin in object1.iter('xmin'):\n",
    "                    x1 = int(float(xmin.text))\n",
    "                for xmax in object1.iter('xmax'):\n",
    "                    x2 = int(float(xmax.text))\n",
    "                for ymin in object1.iter('ymin'):\n",
    "                    y1 = int(float(ymin.text))\n",
    "                for ymax in object1.iter('ymax'):\n",
    "                    y2 = int(float(ymax.text))\n",
    "\n",
    "                img = Image.open('data/VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007/JPEGImages/' + img_name[0:-4] + '.jpg')\n",
    "                img = img.crop((x1,y1,x2,y2)).resize((resnet_input,resnet_input))\n",
    "                if dirname in classes:\n",
    "                    img.save('data/train/' + dirname + '/' + str(counts[classes.index(dirname)]) + \".jpg\");\n",
    "                    counts[classes.index(dirname)]+=1\n",
    "                    \n",
    "    \n",
    "        \n",
    "jamie_bronn_build_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "18 156 53 186\n",
      "0\n",
      "81 62 333 266\n",
      "1\n",
      "153 125 227 144\n",
      "2\n",
      "84 161 493 220\n",
      "3\n",
      "364 61 384 318\n",
      "4\n",
      "63 172 97 491\n",
      "5\n",
      "26 152 161 374\n",
      "6\n",
      "46 116 350 356\n",
      "7\n",
      "251 55 368 55\n",
      "7\n",
      "249 47 333 51\n",
      "8\n",
      "177 0 190 187\n",
      "9\n",
      "484 114 495 323\n",
      "10\n",
      "207 328 246 409\n",
      "11\n",
      "337 89 494 261\n",
      "12\n",
      "126 266 191 358\n",
      "13\n",
      "170 106 327 246\n",
      "14\n",
      "101 18 365 107\n",
      "15\n",
      "209 118 379 270\n",
      "16\n",
      "291 249 432 292\n",
      "17\n",
      "45 26 390 340\n",
      "18\n",
      "154 109 457 216\n",
      "19\n",
      "243 141 467 169\n",
      "20\n",
      "273 331 455 372\n",
      "21\n",
      "172 154 252 156\n",
      "22\n",
      "261 48 292 322\n",
      "23\n",
      "117 237 122 322\n",
      "24\n",
      "293 92 327 319\n",
      "25\n",
      "125 21 365 226\n",
      "26\n",
      "215 350 232 404\n",
      "27\n",
      "175 75 442 322\n",
      "28\n",
      "60 9 177 191\n",
      "29\n",
      "281 54 469 346\n",
      "30\n",
      "48 199 176 336\n",
      "31\n",
      "308 83 372 227\n",
      "32\n",
      "40 113 364 302\n",
      "33\n",
      "234 328 384 344\n",
      "34\n",
      "244 127 276 251\n",
      "35\n",
      "223 71 497 113\n",
      "36\n",
      "236 250 335 301\n",
      "37\n",
      "28 181 365 221\n",
      "38\n",
      "91 148 227 313\n",
      "39\n",
      "8 9 329 172\n",
      "40\n",
      "36 223 224 249\n",
      "40\n",
      "50 196 437 346\n",
      "41\n",
      "71 34 230 72\n",
      "42\n",
      "175 16 212 359\n",
      "43\n",
      "205 156 336 237\n",
      "44\n",
      "154 167 466 186\n",
      "45\n",
      "331 127 402 143\n",
      "46\n",
      "61 90 179 306\n",
      "47\n",
      "223 133 468 241\n",
      "48\n",
      "207 160 324 225\n",
      "49\n",
      "230 64 444 105\n",
      "50\n",
      "60 231 278 299\n",
      "51\n",
      "187 118 459 342\n",
      "52\n",
      "379 183 403 295\n",
      "53\n",
      "32 114 198 164\n",
      "54\n",
      "364 222 439 283\n",
      "55\n",
      "43 114 267 123\n",
      "56\n",
      "37 272 339 316\n",
      "57\n",
      "146 251 324 444\n",
      "58\n",
      "24 206 54 224\n",
      "58\n",
      "135 215 162 225\n",
      "59\n",
      "136 10 259 178\n",
      "60\n",
      "163 152 222 374\n",
      "61\n",
      "217 80 440 117\n",
      "62\n",
      "80 220 405 348\n",
      "63\n",
      "150 88 277 246\n",
      "64\n",
      "232 254 251 330\n",
      "65\n",
      "11 47 184 224\n",
      "66\n",
      "233 198 375 307\n",
      "67\n",
      "241 56 478 230\n",
      "68\n",
      "124 62 263 471\n",
      "69\n",
      "325 311 424 324\n",
      "70\n",
      "260 16 485 254\n",
      "71\n",
      "129 161 210 204\n",
      "72\n",
      "392 236 470 315\n",
      "73\n",
      "96 35 263 96\n",
      "74\n",
      "46 334 363 421\n",
      "75\n",
      "183 82 351 88\n",
      "76\n",
      "22 60 380 165\n",
      "77\n",
      "215 112 257 327\n",
      "78\n",
      "183 418 261 444\n",
      "79\n",
      "28 63 103 185\n",
      "80\n",
      "241 206 326 295\n",
      "81\n",
      "26 121 69 371\n",
      "81\n",
      "222 25 340 180\n",
      "82\n",
      "76 0 107 214\n",
      "83\n",
      "153 32 238 43\n",
      "84\n",
      "132 6 408 279\n",
      "85\n",
      "44 80 47 341\n",
      "86\n",
      "36 142 274 263\n",
      "87\n",
      "284 162 409 361\n",
      "88\n",
      "86 299 319 342\n",
      "89\n",
      "151 107 461 188\n",
      "90\n",
      "253 227 283 308\n",
      "91\n",
      "466 218 496 423\n",
      "92\n",
      "95 277 99 326\n",
      "93\n",
      "94 225 384 254\n",
      "94\n",
      "52 105 142 136\n",
      "95\n",
      "172 38 456 306\n",
      "96\n",
      "262 274 495 278\n",
      "97\n",
      "124 130 494 244\n",
      "98\n",
      "302 45 395 287\n",
      "99\n",
      "148 47 419 334\n",
      "100\n",
      "110 81 171 181\n",
      "101\n",
      "316 181 384 231\n",
      "102\n",
      "63 24 460 119\n",
      "103\n",
      "288 53 293 286\n",
      "104\n",
      "55 104 113 173\n",
      "105\n",
      "107 67 230 257\n",
      "106\n",
      "239 94 289 247\n",
      "107\n",
      "154 232 290 336\n",
      "108\n",
      "79 167 326 424\n",
      "109\n",
      "64 106 285 441\n",
      "110\n",
      "171 238 323 361\n",
      "111\n",
      "399 192 446 349\n",
      "112\n",
      "68 51 427 88\n",
      "113\n",
      "31 314 230 317\n",
      "114\n",
      "202 256 274 295\n",
      "115\n",
      "133 11 246 270\n",
      "116\n",
      "288 121 334 134\n",
      "117\n",
      "56 16 308 164\n",
      "118\n",
      "148 133 224 192\n",
      "119\n",
      "132 98 284 488\n",
      "120\n",
      "7 230 7 238\n",
      "120\n",
      "251 168 330 458\n",
      "121\n",
      "301 78 402 213\n",
      "122\n",
      "215 127 475 188\n",
      "123\n",
      "145 189 153 428\n",
      "124\n",
      "303 124 317 280\n",
      "125\n",
      "0 262 258 347\n",
      "126\n",
      "220 268 336 415\n",
      "127\n",
      "219 145 230 157\n",
      "128\n",
      "374 103 420 214\n",
      "129\n",
      "53 38 98 61\n",
      "130\n",
      "222 40 339 286\n",
      "131\n",
      "277 321 389 356\n",
      "132\n",
      "38 198 250 457\n",
      "133\n",
      "135 21 479 322\n",
      "134\n",
      "156 137 483 140\n",
      "135\n",
      "248 10 321 14\n",
      "136\n",
      "57 241 487 281\n",
      "137\n",
      "15 47 316 123\n",
      "138\n",
      "12 208 142 222\n",
      "138\n",
      "240 298 311 328\n",
      "139\n",
      "178 126 466 160\n",
      "140\n",
      "87 76 201 295\n",
      "141\n",
      "150 216 487 327\n",
      "142\n",
      "345 3 430 168\n",
      "143\n",
      "185 17 310 120\n",
      "144\n",
      "164 117 389 324\n",
      "145\n",
      "23 201 67 274\n",
      "146\n",
      "129 65 344 118\n",
      "147\n",
      "21 231 438 255\n",
      "148\n",
      "233 138 347 302\n",
      "149\n",
      "452 214 488 342\n",
      "150\n",
      "82 249 258 281\n",
      "151\n",
      "83 176 274 239\n",
      "152\n",
      "54 204 266 340\n",
      "153\n",
      "7 156 374 336\n",
      "154\n",
      "84 130 392 319\n",
      "155\n",
      "21 15 341 251\n",
      "156\n",
      "131 24 153 257\n",
      "157\n",
      "49 66 411 107\n",
      "158\n",
      "238 8 411 96\n",
      "159\n",
      "150 6 313 190\n",
      "160\n",
      "49 183 323 190\n",
      "161\n",
      "168 113 258 180\n",
      "162\n",
      "65 120 442 172\n",
      "163\n",
      "82 90 111 325\n",
      "164\n",
      "225 15 307 104\n",
      "165\n",
      "150 272 157 385\n",
      "166\n",
      "117 112 133 446\n",
      "167\n",
      "289 321 375 461\n",
      "168\n",
      "83 180 441 267\n",
      "169\n",
      "257 209 390 233\n",
      "170\n",
      "171 16 310 232\n",
      "171\n",
      "1 159 191 356\n",
      "172\n",
      "153 0 197 382\n",
      "173\n",
      "218 10 302 421\n",
      "174\n",
      "139 285 371 347\n",
      "175\n",
      "294 28 480 149\n",
      "176\n",
      "208 176 222 338\n",
      "177\n",
      "155 180 300 351\n",
      "178\n",
      "96 32 263 206\n",
      "179\n",
      "117 17 333 305\n",
      "180\n",
      "22 250 304 269\n",
      "181\n",
      "32 366 47 452\n",
      "182\n",
      "269 152 369 164\n",
      "183\n",
      "225 42 306 232\n",
      "184\n",
      "156 142 190 363\n",
      "185\n",
      "24 100 487 118\n",
      "186\n",
      "156 195 306 291\n",
      "187\n",
      "191 282 225 291\n",
      "188\n",
      "1 123 390 204\n",
      "189\n",
      "301 7 371 214\n",
      "190\n",
      "61 62 260 313\n",
      "191\n",
      "101 227 184 318\n",
      "192\n",
      "239 111 498 207\n",
      "193\n",
      "178 67 406 122\n",
      "194\n",
      "388 7 453 339\n",
      "195\n",
      "169 34 371 45\n",
      "196\n",
      "188 193 276 247\n",
      "197\n",
      "84 145 141 231\n",
      "198\n",
      "62 72 97 359\n",
      "199\n",
      "47 162 311 285\n",
      "200\n",
      "189 11 230 193\n",
      "201\n",
      "26 31 112 350\n",
      "202\n",
      "118 193 341 331\n",
      "203\n",
      "256 248 353 265\n",
      "204\n",
      "130 131 311 337\n",
      "205\n",
      "409 53 442 149\n",
      "206\n",
      "120 55 252 197\n",
      "207\n",
      "398 303 442 304\n",
      "208\n",
      "35 97 103 185\n",
      "209\n",
      "139 11 249 163\n",
      "210\n",
      "183 210 216 238\n",
      "211\n",
      "41 284 464 344\n",
      "212\n",
      "223 40 403 291\n",
      "213\n",
      "96 44 117 170\n",
      "214\n",
      "2 2 390 80\n",
      "215\n",
      "77 330 355 354\n",
      "216\n",
      "283 81 459 219\n",
      "217\n",
      "147 203 277 257\n",
      "218\n",
      "340 205 476 312\n",
      "219\n",
      "30 278 487 332\n",
      "220\n",
      "363 240 409 324\n",
      "221\n",
      "23 30 261 74\n",
      "222\n",
      "176 174 189 332\n",
      "223\n",
      "12 245 177 368\n",
      "224\n",
      "3 41 346 204\n",
      "225\n",
      "111 146 214 173\n",
      "226\n",
      "366 124 412 130\n",
      "227\n",
      "22 66 154 71\n",
      "228\n",
      "94 48 343 320\n",
      "229\n",
      "184 54 410 208\n",
      "230\n",
      "63 12 92 326\n",
      "231\n",
      "187 153 363 164\n",
      "232\n",
      "239 79 327 331\n",
      "233\n",
      "127 314 330 466\n",
      "234\n",
      "415 99 457 232\n",
      "235\n",
      "25 160 119 254\n",
      "236\n",
      "387 103 445 366\n",
      "237\n",
      "3 173 448 302\n",
      "238\n",
      "238 181 332 285\n",
      "239\n",
      "392 245 492 338\n",
      "240\n",
      "124 129 325 392\n",
      "241\n",
      "89 59 259 257\n",
      "242\n",
      "23 164 241 366\n",
      "243\n",
      "37 16 393 231\n",
      "244\n",
      "12 167 299 283\n",
      "245\n",
      "11 35 291 338\n",
      "246\n",
      "237 138 340 160\n",
      "247\n",
      "181 47 387 163\n",
      "248\n",
      "166 37 193 61\n",
      "249\n",
      "15 158 496 245\n",
      "250\n",
      "92 84 245 103\n",
      "251\n",
      "289 416 300 456\n",
      "252\n",
      "141 251 265 287\n",
      "253\n",
      "31 122 473 255\n",
      "254\n",
      "196 187 373 306\n",
      "255\n",
      "445 162 472 314\n",
      "256\n",
      "50 53 287 89\n",
      "257\n",
      "70 105 480 111\n",
      "258\n",
      "49 100 257 260\n",
      "259\n",
      "320 53 392 216\n",
      "260\n",
      "85 131 94 232\n",
      "261\n",
      "150 244 231 443\n",
      "262\n",
      "59 248 284 370\n",
      "262\n",
      "152 105 206 254\n",
      "263\n",
      "89 155 124 404\n",
      "264\n",
      "238 6 368 261\n",
      "265\n",
      "389 86 417 289\n",
      "266\n",
      "88 34 401 304\n",
      "267\n",
      "195 24 302 29\n",
      "268\n",
      "228 62 303 282\n",
      "269\n",
      "68 63 145 322\n",
      "270\n",
      "457 184 476 224\n",
      "271\n",
      "36 34 150 256\n",
      "272\n",
      "198 80 247 241\n",
      "273\n",
      "64 99 427 332\n",
      "274\n",
      "243 307 253 332\n",
      "275\n",
      "152 97 470 283\n",
      "276\n",
      "172 70 301 169\n",
      "277\n",
      "221 213 437 294\n",
      "278\n",
      "338 246 344 272\n",
      "279\n",
      "342 273 370 312\n",
      "280\n",
      "147 39 304 237\n",
      "281\n",
      "160 162 370 260\n",
      "282\n",
      "284 72 312 331\n",
      "283\n",
      "179 18 353 190\n",
      "284\n",
      "245 37 254 243\n",
      "285\n",
      "93 458 271 466\n",
      "286\n",
      "112 23 179 140\n",
      "287\n",
      "8 50 190 156\n",
      "288\n",
      "275 247 444 303\n",
      "289\n",
      "195 230 214 358\n",
      "290\n",
      "12 92 145 315\n",
      "291\n",
      "318 50 346 389\n",
      "292\n",
      "83 208 251 241\n",
      "293\n",
      "64 150 348 343\n",
      "294\n",
      "111 175 398 324\n",
      "295\n",
      "119 61 369 75\n",
      "296\n",
      "3 230 488 308\n",
      "297\n",
      "101 303 267 493\n",
      "297\n",
      "125 163 494 269\n",
      "298\n",
      "129 27 240 360\n",
      "299\n",
      "457 171 474 263\n",
      "300\n",
      "19 182 178 281\n",
      "301\n",
      "123 49 283 204\n",
      "302\n",
      "244 257 327 258\n",
      "303\n",
      "38 59 283 359\n",
      "304\n",
      "235 64 265 482\n",
      "305\n",
      "60 84 349 255\n",
      "306\n",
      "36 251 78 264\n",
      "307\n",
      "268 146 312 178\n",
      "308\n",
      "31 1 489 113\n",
      "309\n",
      "53 141 366 306\n",
      "310\n",
      "12 48 347 351\n",
      "311\n",
      "445 220 478 275\n",
      "312\n",
      "307 183 435 315\n",
      "313\n",
      "0 136 463 174\n",
      "314\n",
      "490 337 495 352\n",
      "315\n",
      "234 35 495 253\n",
      "316\n",
      "61 314 404 323\n",
      "317\n",
      "55 280 99 322\n",
      "318\n",
      "1 1 486 114\n",
      "319\n",
      "247 124 482 127\n",
      "320\n",
      "20 201 494 368\n",
      "321\n",
      "334 35 465 300\n",
      "322\n",
      "42 230 54 234\n",
      "323\n",
      "145 162 277 364\n",
      "324\n",
      "83 74 227 337\n",
      "325\n",
      "219 138 361 182\n",
      "326\n",
      "135 90 189 109\n",
      "327\n",
      "50 184 282 295\n",
      "328\n",
      "117 267 245 272\n",
      "329\n",
      "37 154 351 172\n",
      "330\n",
      "86 128 164 226\n",
      "331\n",
      "56 33 390 112\n",
      "332\n",
      "188 52 497 362\n",
      "333\n",
      "101 179 107 274\n",
      "334\n",
      "6 207 367 221\n",
      "335\n",
      "78 12 188 201\n",
      "336\n",
      "398 255 456 262\n",
      "337\n",
      "32 69 190 479\n",
      "338\n",
      "218 154 401 165\n",
      "339\n",
      "275 176 346 193\n",
      "340\n",
      "273 255 356 375\n",
      "341\n",
      "150 64 487 220\n",
      "342\n",
      "56 144 343 195\n",
      "343\n",
      "253 223 366 275\n",
      "344\n",
      "343 194 422 216\n",
      "345\n",
      "449 35 464 64\n",
      "346\n",
      "199 275 383 316\n",
      "347\n",
      "274 41 457 275\n",
      "348\n",
      "55 70 415 257\n",
      "349\n",
      "46 118 197 264\n",
      "350\n",
      "16 102 82 489\n",
      "351\n",
      "97 49 232 287\n",
      "352\n",
      "181 266 187 337\n",
      "353\n",
      "96 36 428 261\n",
      "354\n",
      "291 44 400 285\n",
      "355\n",
      "175 195 376 295\n",
      "356\n",
      "211 12 273 127\n",
      "357\n",
      "178 237 182 488\n",
      "358\n",
      "63 194 64 482\n",
      "359\n",
      "174 51 235 383\n",
      "360\n",
      "180 193 414 272\n",
      "361\n",
      "138 253 243 352\n",
      "362\n",
      "66 123 197 138\n",
      "363\n",
      "121 18 233 320\n",
      "364\n",
      "265 215 302 371\n",
      "365\n",
      "160 106 292 158\n",
      "366\n",
      "413 21 467 265\n",
      "367\n",
      "10 218 247 248\n",
      "368\n",
      "50 9 65 58\n",
      "369\n",
      "65 198 330 348\n",
      "370\n",
      "125 32 208 119\n",
      "371\n",
      "109 189 236 388\n",
      "372\n",
      "96 77 305 97\n",
      "373\n",
      "247 255 303 412\n",
      "374\n",
      "157 270 181 335\n",
      "375\n",
      "102 14 272 150\n",
      "376\n",
      "241 29 287 298\n",
      "377\n",
      "100 146 299 499\n",
      "378\n",
      "133 101 272 319\n",
      "379\n",
      "36 109 76 314\n",
      "380\n",
      "104 183 490 200\n",
      "381\n",
      "144 44 422 369\n",
      "382\n",
      "70 15 270 63\n",
      "383\n",
      "12 73 328 290\n",
      "384\n",
      "194 295 405 366\n",
      "385\n",
      "375 63 397 318\n",
      "386\n",
      "374 136 456 223\n",
      "387\n",
      "130 8 294 268\n",
      "388\n",
      "248 174 298 230\n",
      "389\n",
      "35 174 104 315\n",
      "390\n",
      "1 37 124 216\n",
      "391\n",
      "141 374 305 454\n",
      "392\n",
      "309 238 393 254\n",
      "393\n",
      "141 186 341 252\n",
      "394\n",
      "483 14 484 59\n",
      "395\n",
      "198 197 472 342\n",
      "396\n",
      "39 139 223 251\n",
      "397\n",
      "22 191 68 227\n",
      "398\n",
      "84 5 208 26\n",
      "399\n",
      "251 87 347 229\n",
      "400\n",
      "13 74 372 192\n",
      "401\n",
      "155 225 163 255\n",
      "402\n",
      "320 88 461 121\n",
      "403\n",
      "89 295 116 429\n",
      "403\n",
      "301 84 307 149\n",
      "404\n",
      "5 317 112 368\n",
      "405\n",
      "254 202 289 272\n",
      "406\n",
      "320 218 480 242\n",
      "407\n",
      "156 73 275 307\n",
      "408\n",
      "230 274 305 299\n",
      "409\n",
      "97 18 115 147\n",
      "410\n",
      "234 243 316 296\n",
      "411\n",
      "280 245 314 292\n",
      "412\n",
      "31 38 38 160\n",
      "413\n",
      "0 72 101 238\n",
      "414\n",
      "55 67 241 128\n",
      "415\n",
      "96 191 377 228\n",
      "416\n",
      "104 51 319 94\n",
      "417\n",
      "309 40 320 133\n",
      "418\n",
      "222 38 346 278\n",
      "419\n",
      "116 166 234 306\n",
      "420\n",
      "293 339 371 369\n",
      "421\n",
      "40 265 269 345\n",
      "422\n",
      "113 23 247 178\n",
      "423\n",
      "143 76 166 224\n",
      "424\n",
      "104 21 354 134\n",
      "425\n",
      "53 21 385 253\n",
      "426\n",
      "29 30 158 30\n",
      "426\n",
      "43 175 72 222\n",
      "427\n",
      "126 113 317 201\n",
      "428\n",
      "63 56 378 333\n",
      "429\n",
      "251 94 489 207\n",
      "430\n",
      "58 154 161 312\n",
      "431\n",
      "243 152 265 264\n",
      "432\n",
      "125 32 426 316\n",
      "433\n",
      "221 134 472 357\n",
      "434\n",
      "27 62 72 151\n",
      "435\n",
      "224 247 305 427\n",
      "436\n",
      "11 251 153 319\n",
      "437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91 23 186 310\n",
      "438\n",
      "258 59 485 142\n",
      "439\n",
      "63 144 296 186\n",
      "440\n",
      "17 244 191 288\n",
      "441\n",
      "273 76 339 386\n",
      "442\n",
      "377 171 391 221\n",
      "443\n",
      "241 160 462 306\n",
      "444\n",
      "269 185 451 305\n",
      "445\n",
      "27 154 208 374\n",
      "446\n",
      "111 64 119 182\n",
      "447\n",
      "17 168 456 288\n",
      "448\n",
      "356 83 429 317\n",
      "449\n",
      "58 172 146 354\n",
      "450\n",
      "3 15 450 24\n",
      "451\n",
      "109 129 128 199\n",
      "452\n",
      "44 19 111 204\n",
      "453\n",
      "272 90 337 120\n",
      "454\n",
      "75 25 219 48\n",
      "455\n",
      "134 248 178 258\n",
      "456\n",
      "0 11 289 246\n",
      "457\n",
      "194 75 198 285\n",
      "458\n",
      "81 36 484 287\n",
      "459\n",
      "317 176 418 227\n",
      "460\n",
      "27 12 496 247\n",
      "461\n",
      "33 256 309 307\n",
      "462\n",
      "79 282 155 334\n",
      "463\n",
      "33 160 321 261\n",
      "464\n",
      "353 59 369 287\n",
      "465\n",
      "224 71 287 121\n",
      "466\n",
      "79 187 252 359\n",
      "467\n",
      "211 115 298 215\n",
      "468\n",
      "78 108 134 238\n",
      "469\n",
      "47 73 329 332\n",
      "470\n",
      "193 261 212 281\n",
      "471\n",
      "33 302 140 331\n",
      "472\n",
      "86 16 475 63\n",
      "473\n",
      "17 109 238 256\n",
      "474\n",
      "93 143 482 238\n",
      "475\n",
      "172 121 395 270\n",
      "476\n",
      "45 22 444 224\n",
      "477\n",
      "270 29 410 78\n",
      "478\n",
      "17 329 321 481\n",
      "479\n",
      "62 100 143 134\n",
      "480\n",
      "296 106 390 329\n",
      "481\n",
      "153 110 159 119\n",
      "482\n",
      "156 56 408 162\n",
      "483\n",
      "185 153 334 186\n",
      "484\n",
      "396 130 493 304\n",
      "485\n",
      "67 250 189 347\n",
      "486\n",
      "82 60 261 231\n",
      "487\n",
      "127 358 333 365\n",
      "488\n",
      "310 368 331 372\n",
      "489\n",
      "411 258 467 315\n",
      "490\n",
      "79 150 482 253\n",
      "491\n",
      "143 11 372 83\n",
      "492\n",
      "4 109 358 239\n",
      "493\n",
      "34 93 302 293\n",
      "494\n",
      "94 9 95 367\n",
      "495\n",
      "164 60 196 485\n",
      "496\n",
      "200 481 266 487\n",
      "497\n",
      "29 137 31 402\n",
      "497\n",
      "153 10 187 319\n",
      "498\n",
      "108 134 469 301\n",
      "499\n",
      "102 172 327 217\n",
      "500\n",
      "428 18 456 246\n",
      "501\n",
      "65 146 293 199\n",
      "502\n",
      "204 2 308 47\n",
      "503\n",
      "17 185 108 307\n",
      "504\n",
      "272 144 384 372\n",
      "505\n",
      "178 333 238 356\n",
      "506\n",
      "113 174 207 237\n",
      "507\n",
      "287 34 483 183\n",
      "508\n",
      "20 41 310 332\n",
      "509\n",
      "169 6 323 219\n",
      "510\n",
      "57 254 380 271\n",
      "511\n",
      "256 109 391 164\n",
      "512\n",
      "130 144 358 320\n",
      "513\n",
      "3 214 381 269\n",
      "514\n",
      "201 0 326 326\n",
      "515\n",
      "43 92 401 211\n",
      "516\n",
      "89 60 457 204\n",
      "517\n",
      "283 1 372 63\n",
      "518\n",
      "207 159 469 199\n",
      "519\n",
      "49 147 325 284\n",
      "520\n",
      "259 118 431 151\n",
      "521\n",
      "125 63 361 297\n",
      "522\n",
      "96 209 309 210\n",
      "523\n",
      "26 208 82 246\n",
      "524\n",
      "85 150 366 260\n",
      "525\n",
      "54 330 249 493\n",
      "525\n",
      "300 19 305 262\n",
      "526\n",
      "115 27 423 94\n",
      "527\n",
      "17 136 74 235\n",
      "528\n",
      "63 179 179 291\n",
      "529\n",
      "59 214 131 232\n",
      "529\n",
      "54 33 176 134\n",
      "530\n",
      "261 317 466 360\n",
      "531\n",
      "19 195 78 401\n",
      "532\n",
      "34 234 428 389\n",
      "533\n",
      "199 64 416 238\n",
      "534\n",
      "357 23 461 99\n",
      "535\n",
      "363 75 372 184\n",
      "536\n",
      "9 207 296 290\n",
      "537\n",
      "187 280 189 372\n",
      "538\n",
      "60 255 331 322\n",
      "539\n",
      "406 56 414 57\n",
      "540\n",
      "111 9 498 332\n",
      "541\n",
      "103 28 310 79\n",
      "542\n",
      "112 279 369 417\n",
      "543\n",
      "116 97 142 253\n",
      "544\n",
      "12 66 218 192\n",
      "545\n",
      "172 73 493 321\n",
      "546\n",
      "85 4 247 217\n",
      "547\n",
      "172 165 277 496\n",
      "548\n",
      "229 191 474 368\n",
      "549\n",
      "134 54 275 181\n",
      "550\n",
      "61 214 267 225\n",
      "551\n",
      "194 69 481 210\n",
      "552\n",
      "17 146 352 265\n",
      "553\n",
      "11 126 115 270\n",
      "554\n",
      "90 55 434 63\n",
      "555\n",
      "133 309 364 346\n",
      "556\n",
      "40 329 181 382\n",
      "556\n",
      "474 95 484 135\n",
      "557\n",
      "288 29 314 180\n",
      "558\n",
      "227 124 366 265\n",
      "559\n",
      "273 92 434 232\n",
      "560\n",
      "385 222 467 237\n",
      "561\n",
      "265 63 407 259\n",
      "562\n",
      "141 196 252 231\n",
      "563\n",
      "95 5 430 349\n",
      "564\n",
      "8 37 445 274\n",
      "565\n",
      "305 52 476 173\n",
      "566\n",
      "354 162 474 197\n",
      "567\n",
      "127 70 173 346\n",
      "568\n",
      "36 71 285 419\n",
      "569\n",
      "228 119 308 493\n",
      "570\n",
      "5 33 39 118\n",
      "571\n",
      "192 17 369 168\n",
      "572\n",
      "226 66 445 261\n",
      "573\n",
      "57 170 331 227\n",
      "574\n",
      "126 137 405 278\n",
      "575\n",
      "249 434 344 489\n",
      "576\n",
      "146 374 234 375\n",
      "577\n",
      "374 91 487 356\n",
      "578\n",
      "197 218 388 253\n",
      "579\n",
      "61 81 316 384\n",
      "580\n",
      "126 57 379 224\n",
      "581\n",
      "12 155 78 280\n",
      "582\n",
      "57 158 350 369\n",
      "583\n",
      "142 62 157 145\n",
      "584\n",
      "18 27 474 53\n",
      "585\n",
      "133 274 239 321\n",
      "586\n",
      "240 23 324 305\n",
      "587\n",
      "44 65 182 228\n",
      "588\n",
      "114 248 255 294\n",
      "589\n",
      "360 147 415 233\n",
      "590\n",
      "27 389 125 400\n",
      "590\n",
      "137 295 345 348\n",
      "591\n",
      "134 284 412 366\n",
      "592\n",
      "337 131 344 193\n",
      "593\n",
      "204 39 205 374\n",
      "594\n",
      "28 111 361 266\n",
      "595\n",
      "190 140 295 271\n",
      "596\n",
      "106 239 363 259\n",
      "597\n",
      "109 22 321 133\n",
      "598\n",
      "28 131 246 306\n",
      "599\n",
      "44 187 300 218\n",
      "600\n",
      "160 73 169 262\n",
      "601\n",
      "1 281 201 366\n",
      "602\n",
      "214 228 498 275\n",
      "603\n",
      "161 43 424 174\n",
      "604\n",
      "277 208 456 283\n",
      "605\n",
      "53 319 337 374\n",
      "606\n",
      "154 55 266 131\n",
      "607\n",
      "77 77 123 147\n",
      "608\n",
      "52 19 431 45\n",
      "609\n",
      "224 85 276 246\n",
      "610\n",
      "180 123 257 133\n",
      "611\n",
      "282 92 434 243\n",
      "612\n",
      "190 106 230 165\n",
      "613\n",
      "445 233 464 266\n",
      "614\n",
      "314 50 495 113\n",
      "615\n",
      "82 188 106 197\n",
      "616\n",
      "26 109 132 135\n",
      "617\n",
      "181 186 323 297\n",
      "618\n",
      "39 100 88 115\n",
      "619\n",
      "153 131 341 153\n",
      "620\n",
      "222 219 463 257\n",
      "621\n",
      "347 232 370 264\n",
      "622\n",
      "157 223 187 275\n",
      "623\n",
      "31 102 286 332\n",
      "624\n",
      "93 248 138 294\n",
      "625\n",
      "15 11 279 148\n",
      "626\n",
      "16 305 147 306\n",
      "627\n",
      "218 51 493 304\n",
      "628\n",
      "104 59 433 116\n",
      "629\n",
      "1 25 93 35\n",
      "630\n",
      "31 118 308 359\n",
      "631\n",
      "418 77 496 227\n",
      "632\n",
      "56 0 325 74\n",
      "633\n",
      "277 116 294 120\n",
      "634\n",
      "14 121 432 219\n",
      "635\n",
      "213 128 395 257\n",
      "636\n",
      "30 67 204 149\n",
      "637\n",
      "150 192 408 276\n",
      "638\n",
      "384 129 447 253\n",
      "639\n",
      "255 92 276 190\n",
      "640\n",
      "22 319 37 372\n",
      "641\n",
      "365 166 425 356\n",
      "642\n",
      "217 11 463 101\n",
      "643\n",
      "131 220 254 240\n",
      "644\n",
      "174 206 255 269\n",
      "645\n",
      "30 269 320 362\n",
      "646\n",
      "191 244 243 254\n",
      "647\n",
      "24 266 65 344\n",
      "648\n",
      "274 381 283 390\n",
      "649\n",
      "127 115 492 283\n",
      "650\n",
      "78 56 236 302\n",
      "651\n",
      "393 14 421 179\n",
      "652\n",
      "326 59 468 327\n",
      "653\n",
      "408 38 455 387\n",
      "654\n",
      "99 205 336 305\n",
      "655\n",
      "152 100 404 111\n",
      "656\n",
      "158 111 367 189\n",
      "657\n",
      "165 20 224 185\n",
      "658\n",
      "21 49 246 280\n",
      "659\n",
      "82 158 158 216\n",
      "660\n",
      "317 68 435 348\n",
      "661\n",
      "296 42 342 73\n",
      "662\n",
      "200 29 202 171\n",
      "663\n",
      "150 31 188 162\n",
      "664\n",
      "107 35 317 58\n",
      "665\n",
      "115 63 282 113\n",
      "666\n",
      "41 200 112 353\n",
      "667\n",
      "87 141 379 229\n",
      "668\n",
      "361 99 454 176\n",
      "669\n",
      "296 130 418 322\n",
      "670\n",
      "81 292 250 300\n",
      "671\n",
      "263 45 334 159\n",
      "672\n",
      "272 25 357 134\n",
      "673\n",
      "86 89 104 264\n",
      "674\n",
      "87 80 302 249\n",
      "675\n",
      "213 123 304 193\n",
      "676\n",
      "138 91 373 196\n",
      "677\n",
      "373 29 471 496\n",
      "678\n",
      "249 58 489 158\n",
      "679\n",
      "113 171 247 370\n",
      "680\n",
      "339 118 358 312\n",
      "681\n",
      "35 179 268 322\n",
      "682\n",
      "126 172 453 240\n",
      "683\n",
      "85 103 462 157\n",
      "684\n",
      "61 144 449 278\n",
      "685\n",
      "209 95 259 196\n",
      "686\n",
      "100 40 137 294\n",
      "687\n",
      "249 68 471 280\n",
      "688\n",
      "60 33 107 75\n",
      "689\n",
      "61 97 426 157\n",
      "690\n",
      "72 114 173 154\n",
      "691\n",
      "1 231 153 243\n",
      "691\n",
      "211 72 262 145\n",
      "692\n",
      "63 175 226 177\n",
      "693\n",
      "266 54 414 95\n",
      "694\n",
      "72 240 134 328\n",
      "695\n",
      "224 14 388 30\n",
      "696\n",
      "8 289 290 339\n",
      "697\n",
      "19 148 425 208\n",
      "698\n",
      "37 48 40 323\n",
      "699\n",
      "189 48 343 320\n",
      "700\n",
      "8 49 79 258\n",
      "701\n",
      "295 152 307 275\n",
      "702\n",
      "123 70 173 469\n",
      "703\n",
      "349 265 446 367\n",
      "704\n",
      "63 203 430 372\n",
      "705\n",
      "316 144 330 332\n",
      "706\n",
      "317 315 333 330\n",
      "707\n",
      "3 213 263 312\n",
      "708\n",
      "162 263 402 327\n",
      "709\n",
      "82 32 326 38\n",
      "710\n",
      "37 160 265 266\n",
      "711\n",
      "379 149 478 256\n",
      "712\n",
      "55 145 257 477\n",
      "713\n",
      "79 4 317 160\n",
      "714\n",
      "330 229 365 370\n",
      "715\n",
      "220 255 372 329\n",
      "716\n",
      "158 94 233 264\n",
      "717\n",
      "177 107 196 469\n",
      "718\n",
      "3 116 405 151\n",
      "719\n",
      "157 60 244 221\n",
      "720\n",
      "267 298 335 302\n",
      "721\n",
      "156 72 157 300\n",
      "722\n",
      "81 224 120 232\n",
      "723\n",
      "23 12 300 110\n",
      "724\n",
      "58 301 268 313\n",
      "725\n",
      "83 332 289 412\n",
      "725\n",
      "292 134 484 184\n",
      "726\n",
      "177 268 214 274\n",
      "727\n",
      "65 172 395 329\n",
      "728\n",
      "320 217 342 223\n",
      "729\n",
      "88 49 166 156\n",
      "730\n",
      "125 162 220 424\n",
      "731\n",
      "253 73 297 217\n",
      "732\n",
      "106 67 162 368\n",
      "733\n",
      "116 110 125 304\n",
      "734\n",
      "197 34 410 182\n",
      "735\n",
      "169 60 259 204\n",
      "736\n",
      "139 32 306 336\n",
      "737\n",
      "158 356 191 475\n",
      "738\n",
      "27 316 167 322\n",
      "739\n",
      "81 59 329 112\n",
      "740\n",
      "23 12 63 74\n",
      "741\n",
      "150 83 477 109\n",
      "742\n",
      "83 65 198 334\n",
      "743\n",
      "27 114 387 307\n",
      "744\n",
      "180 197 332 294\n",
      "745\n",
      "137 41 139 266\n",
      "746\n",
      "130 71 464 90\n",
      "747\n",
      "0 109 393 328\n",
      "748\n",
      "177 114 258 192\n",
      "749\n",
      "190 138 402 236\n",
      "750\n",
      "221 14 262 108\n",
      "751\n",
      "87 63 140 71\n",
      "752\n",
      "112 19 462 299\n",
      "753\n",
      "15 144 54 209\n",
      "754\n",
      "28 214 204 227\n",
      "755\n",
      "137 195 284 329\n",
      "756\n",
      "53 104 200 327\n",
      "757\n",
      "176 21 360 247\n",
      "758\n",
      "210 257 283 315\n",
      "759\n",
      "263 34 306 230\n",
      "760\n",
      "375 289 439 312\n"
     ]
    }
   ],
   "source": [
    "def build_background_class():\n",
    "    dir_names = os.listdir('data/VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007/Annotations')\n",
    "    average = int(sum(counts[1:])/len(counts[1:]))\n",
    "    average = 761\n",
    "    counts[0] = 0\n",
    "    while(counts[0]!=average):        \n",
    "        print(counts[0])\n",
    "        select = 1\n",
    "        img_name = random.choice(dir_names)\n",
    "        tree = ET.parse('data/VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007/Annotations/' + img_name)\n",
    "        root = tree.getroot()\n",
    "        img = Image.open('data/VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007/JPEGImages/' + img_name[0:-4] + '.jpg')\n",
    "        imsize = img.size;\n",
    "        x1 = random.randrange(imsize[0])\n",
    "        x2 = random.randrange(imsize[0])\n",
    "        y1 = random.randrange(imsize[1])\n",
    "        y2 = random.randrange(imsize[1])\n",
    "        if(x1>x2):\n",
    "            temp = x1\n",
    "            x1=x2\n",
    "            x2=temp\n",
    "        if(y1>y2):\n",
    "            temp = y1\n",
    "            y1=y2\n",
    "            y2=temp\n",
    "        print(x1,y1,x2,y2)\n",
    "        if(x1==x2 or y1==y2):\n",
    "            continue\n",
    "        for object1 in root.iter('object'):\n",
    "            for name1 in object1.iter('name'):\n",
    "                dirname = name1.text\n",
    "            for bndbox in object1.iter('bndbox'):\n",
    "                intersection = 0\n",
    "                for xmin in object1.iter('xmin'):\n",
    "                    xa = int(float(xmin.text))\n",
    "                for xmax in object1.iter('xmax'):\n",
    "                    xb = int(float(xmax.text))\n",
    "                for ymin in object1.iter('ymin'):\n",
    "                    ya = int(float(ymin.text))\n",
    "                for ymax in object1.iter('ymax'):\n",
    "                    yb = int(float(ymax.text))\n",
    "                ra = Rectangle(x1, x2, y1, y2)\n",
    "                rb = Rectangle(xa, xb, ya, yb)\n",
    "                if(area(ra,rb)!=None):\n",
    "                    intersection = area(ra,rb)\n",
    "                union = ((x2-x1)*(y2-y1))+((xb-xa)*(yb-ya))-intersection\n",
    "                if(float(intersection)/union>0.5):\n",
    "                    select = 0\n",
    "\n",
    "        if(select==1):\n",
    "            img = img.crop((x1,y1,x2,y2)).resize((resnet_input,resnet_input))\n",
    "            img.save('data/train/__background__/' + str(counts[0]) + \".jpg\");\n",
    "            counts[0] += 1\n",
    "    \n",
    "build_background_class()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bb_list = []\n",
    "class hound_dataset(torch.utils.data.Dataset): # Extend PyTorch's Dataset class\n",
    "    def __init__(self, root_dir, train, transform=None):\n",
    "        # Begin\n",
    "        \n",
    "        \n",
    "        if (train==True):            \n",
    "            dir1 = '/train/'\n",
    "            dir_names = os.listdir(root_dir + dir1)\n",
    "            img_names=[]\n",
    "            labels = []\n",
    "            count = 0\n",
    "            for c in dir_names:\n",
    "                names = os.listdir(root_dir + dir1 + c)\n",
    "                N = len(names)\n",
    "                for n in range(N):\n",
    "                    img_names.append(str(root_dir + dir1 + c + '/' + names[n]))\n",
    "                    labels += [count]\n",
    "                count+=1           \n",
    "            self.labels = labels \n",
    "         \n",
    "        \n",
    "        \n",
    "        else:\n",
    "            \n",
    "            img_names=[]\n",
    "            dir1 = '/VOCtest_06-Nov-2007/VOCdevkit/VOC2007/JPEGImages'\n",
    "            dir_names = os.listdir(root_dir + dir1)\n",
    "            for name in dir_names:\n",
    "                bb_sub_list = []\n",
    "                img_names.append(str(root_dir + dir1 + '/' + name))\n",
    "                tree = ET.parse('data/VOCtest_06-Nov-2007/VOCdevkit/VOC2007/Annotations/' + name[:-4] + '.xml')\n",
    "                root = tree.getroot()\n",
    "                for object1 in root.iter('object'):\n",
    "                    for name1 in object1.iter('name'):\n",
    "                        class1 = name1.text                        \n",
    "                    for bndbox in object1.iter('bndbox'):\n",
    "                        for xmin in object1.iter('xmin'):\n",
    "                            x1 = int(float(xmin.text))\n",
    "                        for xmax in object1.iter('xmax'):\n",
    "                            x2 = int(float(xmax.text))\n",
    "                        for ymin in object1.iter('ymin'):\n",
    "                            y1 = int(float(ymin.text))\n",
    "                        for ymax in object1.iter('ymax'):\n",
    "                            y2 = int(float(ymax.text))\n",
    "                        if(classes.__contains__(class1)):\n",
    "                            bb_sub_list.append([x1,y1,x2,y2,classes.index(class1)])        \n",
    "                bb_list.append(bb_sub_list)\n",
    "                #print(bb_sub_list)\n",
    "            self.bb_list = bb_list \n",
    "        \n",
    "        self.img_names = img_names\n",
    "        self.train = train\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        # Begin\n",
    "        \n",
    "        return len(self.img_names)        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # Begin      \n",
    "        \n",
    "        \n",
    "        if(self.train==True):\n",
    "            img_pil = Image.open(self.img_names[idx])\n",
    "            return self.transform(img_pil), self.labels[idx]\n",
    "        else:\n",
    "            print(self.bb_list[idx])\n",
    "            return self.img_names[idx] , self.bb_list[idx]       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "as2\n"
     ]
    }
   ],
   "source": [
    "print('as'+str(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network\n",
    "<br/>You can ask Arya to train the network on the created dataset. This will yield a classification network on the 21 classes of the VOC dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "composed_transform = transforms.Compose([transforms.Scale((resnet_input,resnet_input)),\n",
    "                                         transforms.ToTensor()])\n",
    "#                                          transforms.RandomHorizontalFlip()])\n",
    "train_dataset = hound_dataset(root_dir='./data', train=True, transform=composed_transform) # Supply proper root_dir\n",
    "test_dataset = hound_dataset(root_dir='./data', train=False, transform=composed_transform) # Supply proper root_dir\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[48, 240, 195, 371, 12], [8, 12, 352, 498, 15]]\n",
      "[[139, 200, 207, 301, 19]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[[48, 240, 195, 371, 12], [8, 12, 352, 498, 15]], [[139, 200, 207, 301, 19]]]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataiter = iter(test_loader)\n",
    "test_images, test_labels = test_dataiter.next()\n",
    "bb_list[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    \n",
    "print(type(test_images[0]))\n",
    "img = Image.open(test_images[0])\n",
    "for i in range()\n",
    "box = [100,100,200,200]\n",
    "img.crop(box)\n",
    "#imshow(Image.open(test_images[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4, 5], [2, 3, 5, 4, 7]]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb_list = []\n",
    "bb_list.append([[1,2,3,4,5],[2,3,5,4,7]])\n",
    "bb_list.append([[1,2,3,4,5],[2,3,5,4,7]])\n",
    "bb_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning\n",
    "Litlefinger has brought you a pre-trained network. Fine-tune the network in the following section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (30) : unknown error at /opt/conda/conda-bld/pytorch_1502009910772/work/torch/lib/THC/THCGeneral.c:70",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-141-c8c53934a967>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mresnet18\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresnet18\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m21\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mresnet18\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Add code for using CUDA here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vineeth/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device_id)\u001b[0m\n\u001b[1;32m    145\u001b[0m                 \u001b[0mcopied\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \"\"\"\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vineeth/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vineeth/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    122\u001b[0m                 \u001b[0;31m# Variables stored in modules are graph leaves, and we don't\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                 \u001b[0;31m# want to create copy nodes, so we have to unpack the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m                 \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                     \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vineeth/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    145\u001b[0m                 \u001b[0mcopied\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \"\"\"\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vineeth/anaconda3/lib/python3.6/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, async)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vineeth/anaconda3/lib/python3.6/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_new\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_lazy_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m     \u001b[0;31m# We need this method only for lazy init, so we can remove it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0m_CudaBase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vineeth/anaconda3/lib/python3.6/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m             \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[1;32m     84\u001b[0m     \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_sparse_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0m_cudart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_cudart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (30) : unknown error at /opt/conda/conda-bld/pytorch_1502009910772/work/torch/lib/THC/THCGeneral.c:70"
     ]
    }
   ],
   "source": [
    "resnet18 = models.resnet18(pretrained=True)\n",
    "\n",
    "for param in resnet18.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "resnet18.fc = nn.Linear(resnet18.fc.in_features, 21)\n",
    "resnet18.cuda()\n",
    "\n",
    "# Add code for using CUDA here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "# Update if any errors occur\n",
    "optimizer = optim.SGD(resnet18.fc.parameters(), learning_rate, hyp_momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def arya_train():\n",
    "    # Begin\n",
    "    for epoch in range(num_epochs): \n",
    "        resnet18.train()        \n",
    "        for i, (input1,target) in enumerate(train_loader):\n",
    "            \n",
    "            input_var = torch.autograd.Variable(input1).cuda()            \n",
    "            target = target.cuda(async = True)\n",
    "            target_var = torch.autograd.Variable(target)\n",
    "            \n",
    "            output = resnet18(input_var)            \n",
    "            loss = criterion(output, target_var)           \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            output = output.float()\n",
    "            loss = loss.float()\n",
    "            \n",
    "            print(str(epoch)+ '---'+str(i) + '--------' +  str(loss) + '-------' +str(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time arya_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing and Accuracy Calculation\n",
    "Jorah then asks a question, how is this a detection task?<br/>\n",
    "As everybody wonders, Theon Greyjoy suggests a slding window method to test the above trained trained network on the detection task:<br/>\n",
    "\"We take some windows of varying size and aspect ratios\", he mumbled, \"and slide it through the test image (considering some stride of pixels) from left to right, and top to bottom, detect the class scores for each of the window, and keep only those which are above a certain threshold value!\". \"He is right\", says Samwell, \"I read a similar approach in the paper -Faster RCNN by Ross Girshick in the library, where he uses three diferent scales/sizes and three different aspect ratios, making a total of nine windows per pixel to slide\". You need to write the code and use it in testing code to find the predicted boxes and their classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def theon_sliding_window():\n",
    "    # Begin\n",
    "    boxes=[]\n",
    "    imres = [[192,192],[192,96],[96,192],[128,128],[128,64],[64,128],[64,64],[64,32],[32,64]]\n",
    "    for i in range(resnet_input):\n",
    "        for j in range(resnet_input):\n",
    "            for window in imres:\n",
    "                boxes.append([i-window[0]/2, j-window[1]/2, i+window[0]/2, j+window[1]/2])\n",
    "    return np.asarray(np.array(boxes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Wait\", says <b>Jon Snow</b>, \"The predicted boxes may be too many and we can't deal with all of them. So, I myself will go and apply non_maximum_supression to reduce the number of boxes\". You are free to choose the threshold value for non maximum supression, but choose wisely [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def aegon_targaryen_non_maximum_supression(boxes,threshold = 0.3):\n",
    "            \n",
    "        pick = []\n",
    "\n",
    "        x1 = boxes[:,0]\n",
    "        y1 = boxes[:,1]\n",
    "        x2 = boxes[:,2]\n",
    "        y2 = boxes[:,3]\n",
    "\n",
    "        area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "        idxs = np.argsort(y2)\n",
    "        \n",
    "        while len(idxs) > 0:\n",
    "            \n",
    "            last = len(idxs) - 1\n",
    "            i = idxs[last]\n",
    "            pick.append(i)\n",
    "            suppress = [last]\n",
    "            \n",
    "            for pos in xrange(0, last):\n",
    "                \n",
    "                j = idxs[pos]\n",
    "               \n",
    "                xx1 = max(x1[i], x1[j])\n",
    "                yy1 = max(y1[i], y1[j])\n",
    "                xx2 = min(x2[i], x2[j])\n",
    "                yy2 = min(y2[i], y2[j])\n",
    "               \n",
    "                w = max(0, xx2 - xx1 + 1)\n",
    "                h = max(0, yy2 - yy1 + 1)\n",
    "\n",
    "                overlap = float(w * h) / area[j]\n",
    "\n",
    "                if overlap > threshold:\n",
    "                    suppress.append(pos)\n",
    "\n",
    "            idxs = np.delete(idxs, suppress)\n",
    "\n",
    "        return boxes[pick]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daenerys, the queen, then orders her army to test out the trained model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daenerys_test(resnet18):\n",
    "    # Write loops for testing the model on the test set\n",
    "    # Also print out the accuracy of the model\n",
    "    total_bbox = 0\n",
    "    total_correct_bbox = 0\n",
    "    sliding_windows = theon_sliding_window()\n",
    "    sliding_windows = aegon_targaryen_non_maximum_supression(sliding_windows)\n",
    "    for i, (input1,bb_list1) in enumerate(test_loader):\n",
    "            img_n = -1\n",
    "            for img_name in input1:\n",
    "                img_n+=1\n",
    "                img = Image.open(img_name)\n",
    "                img = img.resize((resnet_input,resnet_input))\n",
    "                total = len(bb_list[i*batch_size + img_n])\n",
    "                total_bbox += total\n",
    "                count = [0]*total\n",
    "                for window in sliding_windows:\n",
    "                    total = len()\n",
    "                    input_conv = img.crop(window).resize((resnet_input,resnet_input))\n",
    "                    input_var = torch.autograd.Variable(input_conv, volatile = True).cuda()                \n",
    "                    output = resnet18(input_var)\n",
    "                    output = output.float()\n",
    "                    pred = output.max(1)[1].cpu().data.numpy()\n",
    "                    for bbox in bb_list[i*batch_size + 1]:\n",
    "                        ra = Rectangle(bbox[0], bbox[1], bbox[2], bbox[3])\n",
    "                        rb = Rectangle(window[0], window[1], window[2], window[3])\n",
    "                        union = (window[3]-window[1])*(window[2]-window[0]) + (bbox[3]- bbox[1])*(bbox[2]- bbox[0])\n",
    "                        if (area(ra, rb) > 0.5*union and pred == bbox[4]):\n",
    "                            count[bb_list[i*batch_size + 1].index(bbox)] = 1\n",
    "                total_correct_bbox += sum(count)\n",
    "    accuracy = total_correct_bbox / total_bbox *100\n",
    "    print('Acuuracy: '+str(accuracy))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'resnet18' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-140-9e63c45cfe71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time daenerys_test(resnet18)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/vineeth/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mmagic\u001b[0;34m(self, arg_s)\u001b[0m\n\u001b[1;32m   2156\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2157\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2158\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2160\u001b[0m     \u001b[0;31m#-------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vineeth/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line)\u001b[0m\n\u001b[1;32m   2077\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2078\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2079\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2080\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-59>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/home/vineeth/anaconda3/lib/python3.6/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vineeth/anaconda3/lib/python3.6/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'resnet18' is not defined"
     ]
    }
   ],
   "source": [
    "%time daenerys_test(resnet18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Showdown\n",
    "After covering all the steps and passing the accuracy value to the talking crystal, they all pass through to the land of the living, with a wounded Jon Snow armed with the Dragon-axe. After a fierce battle, Jon Snow manages to go face to face with the Night king. Surrounded by battling men and falling bodies, they engage in a ferocious battle, a battle of spear and axe. After a raging fight, Jon manages to sink the axe into the Night king's heart, but not before he gets wounded by the spear. As dead men fall to bones, Daenerys and others rush to his aid, but it is too late. Everyone is in tears as they look towards the man of honour, Jon Snow, lying in Daenerys's arms when he says his last words: \"The night has ended. Winter is finally over!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
